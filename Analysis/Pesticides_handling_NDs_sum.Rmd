---
title: "Analysis of Sediment Toxicity Data : Pesticides"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "7/16/2020"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---

<img
  src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
  style="position:absolute;top:10px;right:50px;" />

# Install Libraries

```{r load_library}
library(readxl)
library(tidyverse)
library(GGally)
library(maxLik)

library(CBEPgraphics)
load_cbep_fonts()

library(LCensMeans)
```

# Load Basic Data
```{r}
sibfldnm <- 'Data'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'working_data.xls'

the.data <- read_excel(paste(sibling,fn, sep='/'), 
    sheet = "Combined", col_types = c("skip", 
        "text", "skip", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "text", "text", "numeric", "text", 
        "numeric", "text", "numeric", "numeric", 
        "text", "text", "text", "skip", 
        "skip", "skip", "skip", "skip", 
        "numeric", "numeric", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "skip")) %>%
  mutate(SAMPLE_ID = factor(SAMPLE_ID, levels = c("CSP-1", "CSP-2", "CSP-3", 
                                                  "CSP-4", "CSP-5", "CSP-6",
                                                  "CSP-7", "CSP-7D", "CSP-8", 
                                                  "CSP-9", "CSP-10", "CSP-11",
                                                  "CSP-12", "CSS-13", "CSP-14",
                                                  "CSS-15")))
```

# Chemical Parameters
Pesticide Anylates are listed in a separate tab in the original Excel 
spreadsheet.  Here we pull them into a vector, for later use.
```{r parameter_names, warning=FALSE}
tmp<- read_excel(paste(sibling,fn, sep='/'), sheet = "Pesticides", skip = 3)
Pesticides.names <- tmp %>%
  select(1) %>%
  slice(1:19) %>%
 pull(PARAMETER_NAME)
Pesticides.names
rm(tmp)
```

# Extract Pesticides Data
We  filter down to selected parameters defined by that list of parameters.  The 
second and third filters remove QA/QC samples.  We have some duplicate samples:
1. For CSP-6, CSP-15 - all measurements 
2. For CSP-10 - TOC data only
3. CSP-9 - all observations EXCEPT TOC.
To deal with duplicate samples, we calculate means.

The following code  reads in data and assigns the value of the reporting limit 
to data that was below the detection limits. This sets us up for later analysis
based on different assumptions about how to handle the non-detects.

```{r pesticide_data}
Pesticides.data.long <- the.data %>%
  filter (the.data$PARAMETER_NAME %in% Pesticides.names) %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(CONCENTRATION = ifelse(is.na(CONCENTRATION) & LAB_QUALIFIER == 'U', 
                                REPORTING_LIMIT, CONCENTRATION)) %>%
  group_by(SAMPLE_ID, PARAMETER_NAME) %>%
  summarize(CONCENTRATION = mean(CONCENTRATION, na.rm=TRUE),
            censored = sum(LAB_QUALIFIER=='U', na.rm=TRUE),
            .groups = 'drop') %>%
  ungroup()
```

# Frequency of Detects by Parameter
```{r find_non_detects}
Pesticides.data.long %>%
  group_by(PARAMETER_NAME) %>%
  summarize(n = n(),
            detects = sum(censored==0),
            nondetects = sum(censored>0))
```

So, most pesticides were never detected.  The exceptions include:
`4,4'-DDD`, `4,4'-DDE`, `4,4'-DDT`, `CIS-CHLORDANE`, `TRANS-NONACHLOR`
But even several of those were only detected once, which suggests the analytic 
chemistry was not well suited to dirty harbor sediments.

```{r pesticide_data_smaller}
Pesticides.data <- Pesticides.data.long %>%
  select(-censored) %>%
  filter(PARAMETER_NAME %in% c("4,4'-DDD", "4,4'-DDE", "4,4'-DDT",
                               "CIS-CHLORDANE", "TRANS-NONACHLOR")) %>%
  spread(key = PARAMETER_NAME, value = CONCENTRATION) %>%
  rowwise() %>%
  mutate(totPesticides = sum(c(`4,4'-DDD`, `4,4'-DDE`, `4,4'-DDT`,
                               `CIS-CHLORDANE`, `TRANS-NONACHLOR`), na.rm = TRUE))
```

# Inital Pairs Plot
```{r pairs_plot}
ggpairs(log(Pesticides.data[2:6]), progress=FALSE)
```

Highly skewed data, even after log transformation.  Some moderate correlations
evident.

# A Graphic Check for Reasonableness
```{r graphic check}
tmp <- Pesticides.data.long %>%
  group_by(SAMPLE_ID) %>%
  summarize(totPesticides = sum(CONCENTRATION, na.rm = TRUE),
            countPesticides = sum(censored==0, na.rm=TRUE))

plt <- ggplot(tmp, aes(totPesticides, countPesticides)) +
  geom_point() +
  geom_text(aes(label = SAMPLE_ID),nudge_y = .2, nudge_x = .075) +
  geom_smooth() +
  scale_x_log10() +
  xlab('Log Total Pesticides (ppb)') +
  ylab('Number of Pesticides observed') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(panel.grid = element_blank()) 
plt
```

So, unlike for the PCBs, the inordinately high detection limits for CSP-8 do not
appear as a wild outlier. Presumably, this is because the detection limits are
an order of magnitude lower here than for the PCBs. However, it is notable that
the site still has the highest sum of pesticides (based here on assuming the
"real" concentration is the detection limit).  For consistency, we could
consider removing CSP-8 from the data, but for now we don't do so.

# Initial graphics
```{r concentration_by_site}
tmp <- Pesticides.data
tmp$SAMPLE_ID <- reorder(tmp$SAMPLE_ID, tmp$totPesticides, mean, na.rm = TRUE)

tmp <- tmp %>% gather(key = 'pesticide', value = 'Concentration', 2:6) %>%
  mutate(pesticide = factor(pesticide)) %>%
  mutate(pesticide = reorder(pesticide, Concentration, function(x) -x[1]))
 
  
plt <- ggplot(tmp, aes(x=as.numeric(SAMPLE_ID), y = Concentration, 
                       color = pesticide)) +
  geom_line(lwd = 2) +
  scale_x_continuous(breaks = c(1:16-0.25), labels = levels(tmp$SAMPLE_ID), 
                     minor_breaks = FALSE) +
  #scale_color_discrete(labels = substr(levels(tmp$Pesticides), 0, 
      #gregexpr(pattern ='.,',levels(tmp$Pesticides) ) )) +
  xlab('Site ID') +
  ylab('Log10 of Concentration (ppb)') +
  scale_y_log10() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(panel.grid = element_blank()) 
plt
```

##Sites by Pesticides
```{r concentration_by_compound} 
tmp <- Pesticides.data

tmp <- tmp %>% gather(key = 'pesticide', value = 'Concentration', 2:6) %>%
  mutate(pesticide = factor(pesticide)) %>%
  mutate(pesticide = reorder(pesticide, Concentration, function(x) -x[1]))

plt <- ggplot(tmp, aes(pesticide, Concentration)) +
  geom_col(aes(fill = SAMPLE_ID)) +
  xlab('Site ID') +
  ylab('Concentration (ppb)') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(panel.grid = element_blank()) 
plt
```

So, real Pesticide data is dominated by the DDT residues.  Other compounds
appear high at CSP5 and CSP8, but only because of high levels of non-detects.
We only need to look at  DDT residues.  NOAA's SQUIRTS include screening levels
for the sum of the three main DDT residues.

# Screening Levels
The SQUIRTS for the sum of the three DDT residues are as follows;
```{r squirts}
type   <- c('TEL', 'ERL', 'PEL', 'ERM', 'AET')
levels <- c(3.89, 1.58,51.7, 46.1, 11)
```

Note that the SQUIRTS have screening levels for "Chlordane" but not 
"cis-Chlordane", and no screening values for Nonachlor.

# Handling Non-detects
The preceding plots were based on modeling the value of non-detects as equal to
the Reporting Limit, which is a very conservative approach.  Since detection
limits for all organic contaminants are correlated as reported by the
laboratory, this is problematic, especially for CSP-8, which had unusually high
detection limits.

A better approach uses the available data to estimate values of censored data
based on available data -- including knowledge of detection limits and whether
pesticides were detected.

## Distributional graphics
What kind of distribution do we actually have?

```{r violin_plot}
plt <- ggplot(Pesticides.data.long, aes(PARAMETER_NAME, CONCENTRATION)) +
  geom_violin() +
  geom_point(aes(color = censored>0), alpha = 0.2, size=2) +
  scale_color_manual(name = 'Non Detects', values = cbep_colors()) +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle=90))
plt
```

So, if we focus on the DDT residues, we actually have pretty good data, with
only limited non-detects, and pretty close to a long-normal distribution (as far
as we can tell with these very limited data).

# Alternate Estimates of Sum of DDT Residues
We want to focus on analysis of the sum of DDT residues, in part because the
residues are highly correlated.  The question is, how do we best handle
non-detects?  Often in environmental analyses, non-detects are replaced by zero,
by the detection limit, or by half the detection limit, but none of those
conventions rests on strong statistical principals.  We instead implement a
method that estimates the (unobserved) value of non-detects using a conditional
mean of censored observations derived from a maximum likelihood procedure.

The idea is to fit a maximum likelihood model to the data assuming a censored 
lognormal distribution.  With a lognormal density in hand, we can estimate a 
conditional mean of for "unobserved" observations below a the detection limit by 
sampling from the underlying lognormal distribution 1000 times (or more) and 
calculating a mean.

We developed functions for implementing this procedure.  Those functions have
been incorporated into a small package, 'LCensMeans' to facilitate use in CBEP 
State of the Bay Analyses.

See the help files in the LCensMeans package for more explanation, or read the
R Notebook "Conditional Means of Censored Distributions", where we developed the 
basic approach.

The LCenMeans package is in active development in pre-release form, so there is 
no guarantee that the user interface will not change.  The following code worked 
as of July 2020.

## Applying to the DDT Residues data
Note the use of "mutate"  after the group_by() so that the dataframe is not 
collapsed to the grouping variables, as it would be by summary().

The calculations involved are random, so if you want to get exactly the same 
results, you need to set the random number seed with set.seed()

```{r handle_NDs}
dat2 <- Pesticides.data.long %>%
  filter(PARAMETER_NAME %in% c("4,4'-DDD", "4,4'-DDE", "4,4'-DDT")) %>%
  group_by(PARAMETER_NAME) %>%
  mutate(LikCensored = sub_cmeans(CONCENTRATION, censored>0)) %>%
  mutate(HalfDL = ifelse(censored>0, CONCENTRATION/2, CONCENTRATION)) %>%
  ungroup()

res2 <- dat2 %>%
  group_by(SAMPLE_ID) %>%
  summarize(LNtotPesticide = sum(LikCensored),
            halfDLtotPesticide = sum(HalfDL),
            totPesticide = sum(CONCENTRATION),
            .groups = 'drop')
```

```{r plot_ND_alternatives} 
ggplot(dat2, aes(CONCENTRATION,LikCensored)) + geom_line() + geom_point(aes(color = censored>0), alpha = 0.5) + 
  geom_abline(intercept = 0, slope= 1, alpha = 0.5, color = 'red') + 
facet_wrap('PARAMETER_NAME', scales = 'free', nrow=3)
```

So, for these parameters, the impact of using maximum likelihood estimated is 
very small.

# Final Graphic
```{r}
ggplot(res2, aes(x=totPesticide))+
  geom_point(aes(y=LNtotPesticide), color = 'orange') +
  geom_text(aes(y=LNtotPesticide, label = SAMPLE_ID), color = 'orange', hjust = 0) +
  geom_point(aes(y=halfDLtotPesticide), color = 'red') +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_text(aes(x=200, y=210, label = '1:1 Line'), angle = 35) +
  geom_hline(yintercept = 1.58, color = 'blue', lty=3, size=1) +    #ERL
  geom_text(aes(x=250, y=10, label = 'ERL'), color = 'blue') +
  geom_hline(yintercept = 46.1, color = 'blue', lty=3, size=1) +     #ERM
  geom_text(aes(x=250, y=60, label = 'ERM'), color = 'blue') +
  xlab('Total, Assuming Detection Limit') +
  ylab('Total, Assuming half DL (red) or Max. Lik (orange)') +
  xlim(c(0,375)) +
  scale_x_log10() + scale_y_log10() +
  theme_minimal()
```

So, results are similar, except for CSP-6, where the choice of estimator affects
whether you determine that the likely levels of DDT residues exceeds ERL. 

# Conclusion
We should present results of pesticides restricted only to the sum of the DDT 
breakdown products.

# Correcting for Half non-detects
DDT samples from CSS-15 consisted of one non-detect and one elevated 
observation.  it's not obvious how to handle this site, as the two samples were 
sufficiently different to fail as a QA/QC check.  Here we report the average of 
the two values, although that presents some problems.

When we average across the two values, we are averaging a non-detect with a 
significantly higher observation. In the preceding analysis,  we flagged the 
average as a censored value, below the average.  That may bias the value for 
certain analyses.

In our preliminary analyses (above), we saw that pesticide levels for CSS-15 
were relatively low. If you replace the ND with the "detection limit" -- which 
here would be an average of the observed concentration and the DL -- that value 
slightly exceeds ERL. Using either half the DL or maximum likelihood estimation, 
the sample lies below ERL.

So. how we handle this split sample matters. 

To be thorough, we need to calculate our censored estimates (ND, Half ND and 
Maximum Likelihood) on the original raw data and average the results, rather 
than just assume everything works out o.k. when applied to the averaged value.

Obviously,if we are looking at the full detection limit, the average of a sum 
is the sum of the averages, and it makes no difference, but it should matter 
for the other two estimators, especially for the maximum liklihood estimator.

ML estimator on the non-detect and average that result with the detected value,
rather than applying the ML estimator to the averaged value.  I can't imagine 
it will make much difference, but....

### Raw Observations
```{r}
the.data %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC')
```
## So, correct values
```{r}
(0.744 +0.382)/2
(0.744 + (0.382/2))/2
```

### Calculate half DL limits both ways
```{r}
tmp <- the.data %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(censored = LAB_QUALIFIER %in% c('U', 'J')) %>%
  mutate(CONCENTRATION = ifelse(censored, REPORTING_LIMIT, CONCENTRATION)) %>%
  select(censored, CONCENTRATION, REPORTING_LIMIT) %>%
  mutate(halfdl = ifelse(censored, REPORTING_LIMIT/2, CONCENTRATION))
mean(tmp%>%pull(CONCENTRATION))/2
mean(tmp%>%pull(halfdl))
```
So, doing this right sharply increases our estimate.

### Calculate Maximum Likelihood Estimator two ways
First the correct way, by averaging the two estimates
```{r}
est <- the.data %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  select(SAMPLE_ID, CONCENTRATION, REPORTING_LIMIT, LAB_QUALIFIER) %>%
  mutate(censored = LAB_QUALIFIER %in% c('U', 'J')) %>%
  mutate(CONCENTRATION = ifelse(censored, REPORTING_LIMIT, CONCENTRATION)) %>%
  mutate(lnest = sub_cmeans(CONCENTRATION, censored)) %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  pull(lnest)
est
mean(est)
```

Second, the simple way, by calculating estimates based on average value.

```{r}
Pesticides.data.long %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  mutate(censored = censored>0) %>%
  mutate(lnest = sub_cmeans(CONCENTRATION, censored)) %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  pull(lnest)
```

So, this makes a big difference for our ML estimate, as suspected..
