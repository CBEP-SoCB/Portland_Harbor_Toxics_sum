---
title: "Exploratory analysis of Sediment Toxicity Data : PAHs"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "7/16/2020"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---

<img
  src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
  style="position:absolute;top:10px;right:50px;" />

# Install Libraries
```{r load_libraries}
library(readxl)
library(tidyverse)
library(GGally)  # here, provides pairs plot and parallel Coordinated Plot
library(maxLik)  # Simple Maximum likelihood estimation from arbitrary
                 # Likelihood functions.

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())

library(LCensMeans)
```

# Load Data
```{r load_data}
sibfldnm <- 'Data'
parent <- dirname(getwd())
sibling <- file.path(parent, sibfldnm)

fn <- "working_data.xls"

the.data <- read_excel(paste(sibling, fn, sep='/'), 
    sheet = "Combined", col_types = c("skip", 
        "text", "skip", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "text", "text", "numeric", "text", 
        "numeric", "text", "numeric", "numeric", 
        "text", "text", "text", "skip", 
        "skip", "skip", "skip", "skip", 
        "numeric", "numeric", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "skip")) %>%
  mutate(SAMPLE_ID = factor(SAMPLE_ID, levels = c("CSP-1", "CSP-2", "CSP-3", 
                                                  "CSP-4", "CSP-5", "CSP-6",
                                                  "CSP-7", "CSP-7D", "CSP-8", 
                                                  "CSP-9", "CSP-10", "CSP-11",
                                                  "CSP-12", "CSS-13", "CSP-14",
                                                  "CSS-15")))
```

## List Chemical Parameters
PAH Anylates are listed in a separate tab in the original Excel spreadsheet.  
Here we pull them into a vector, for later use.
```{r load_parameter_names, warning = FALSE}
pah.names <- read_excel(paste(sibling, fn, sep='/'),
                         sheet = "PAHs", skip = 3) %>%
  select(1) %>%
  slice(1:16)
(pah.names <- pah.names$PARAMETER_NAME)
```

## Extract PAH Data
We filter down to selected parameters defined in the list of PAH names.  The 
second and third filters remove QA/QC samples.

We have some duplicate samples:
1.  CSP-6, CSP-15 - all measurements
2,  CSP-10 - TOC  only
3.  CSP-9 - all observations EXCEPT TOC
To address duplicate samples, we calculate average values.

In addition, the following reads in data and assigns the value of the reporting 
limit to data that was below that limit.  Since we retain a flag value 
indicating which observation we treated this way, this does NOT imply that the 
detection limit is the most appropriate estimate of the (unobserved) left 
censored value.

We handle censored values in more detail, below.
```{r pah_data, warning=FALSE}
pah.data.long <- the.data %>%
  filter (the.data$PARAMETER_NAME %in% pah.names) %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(CONCENTRATION = ifelse(is.na(CONCENTRATION) & LAB_QUALIFIER == 'U', 
                                REPORTING_LIMIT, CONCENTRATION)) %>%
  group_by(SAMPLE_ID, PARAMETER_NAME) %>%
  summarize(CONCENTRATION = mean(CONCENTRATION, na.rm=TRUE),
            censored = sum(LAB_QUALIFIER=='U', na.rm=TRUE),
            .groups = 'drop') %>%
  ungroup() %>%
  rename(PAH = PARAMETER_NAME) %>%
  mutate(PAH = factor(PAH)) %>%
  mutate(PAH = reorder(PAH, CONCENTRATION, function(x) mean(x, na.rm=TRUE)))

pah.data <- pah.data.long %>%
  spread(key = PAH, value = CONCENTRATION) %>%
  rowwise() %>%
  mutate(totpah = sum(c(ACENAPHTHENE, ACENAPHTHYLENE, `BENZ(A)ANTHRACENE`,
                        `BENZO(A)PYRENE`, `BENZO(B)FLUORANTHENE`,
                        `BENZO(GHI)PERYLENE`, `BENZO(K)FLUORANTHENE`,
                        CHRYSENE, `DIBENZ(A,H)ANTHRACENE`, FLUORANTHENE, 
                        FLUORENE, `INDENO(1,2,3-CD)PYRENE`, NAPHTHALENE, 
                        PHENANTHRENE,  PYRENE), na.rm = TRUE))
```

# Exploratory Analysis
## Pairs Plot
```{r pairsplot, fig.width = 10, fig.height=8, warning=FALSE}
ggpairs(log(pah.data[3:16]), progress=FALSE, na.rm=TRUE)
```
Observations:  

1.  That shows the very high correlation among all the PAHs. We need to make 
sure that's not due to hidden correlations of detection limits.  

2.  Distributions are not too far from lognormal, although still slightly 
skewed.

## Check for Non-detects
There are only a few.
```{r test_NDs}
pah.data.long %>% select(SAMPLE_ID, PAH, censored) %>%
  group_by(PAH) %>%
  summarize(detects = sum(censored==0, na.rm=TRUE),
            nondetects = sum(censored>0, na.rm=TRUE),
            .groups='drop')
```
This is just another way of showing the high level of correlation among chemical
parameters.  If a sample is high in one PAH, it tends to be high in most / all.

## Graphic
```{r inital_graphic, fig.width = 7, fig.height = 5}
tmp <- pah.data.long %>%
  select(-censored) %>%
  mutate(SAMPLE_ID = fct_reorder(SAMPLE_ID, 
                                 CONCENTRATION, function(x) mean(-x))) %>%
  mutate(PAH = fct_reorder(PAH, CONCENTRATION, function(x) mean(-x)))

plt <- ggplot(tmp, aes(x=as.numeric(SAMPLE_ID), 
                       y = CONCENTRATION, color = PAH)) +
  geom_line(lwd = 1) +
  scale_x_continuous(breaks = c(1:16-0.25), labels = levels(tmp$SAMPLE_ID), 
                     minor_breaks = FALSE) +
  scale_y_log10() +
  xlab('Site ID') +
  ylab('Concentration (ppb)') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(panel.grid = element_blank()) 
plt
```

## Correlations
Quantitative analysis confirms high correlations. Results are similar with 
Kendall's Tau or Pearsons' r.
```{r spearman_correlations }
knitr::kable(cor(pah.data[3:18],
                 use='pairwise',
                 method = 'spearman'), digits = 2)
```

# Analysis of Non Detects
Here we continue the analysis, with an explicit effort to model non-detects.  
We do this, as elsewhere, using a simple maximum likelihood estimation procedure 
to calculate an estimated distribution for each contaminant, and then replace 
the NDs with an estimate of the conditional mean suggested by those 
distributions.

## Distribution Graphic showing NDs
What kind of distribution do we actually have?
```{r violin_plot}
plt <- ggplot(pah.data.long, aes(PAH, CONCENTRATION)) +
  geom_violin() +
  geom_point(aes(color = censored>0), alpha = 0.52, size=2) +
  scale_y_log10(labels = scales::comma) +
  scale_color_manual(name = "Non Detects", values = cbep_colors()) +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust = 0.2))
plt
```
So, only a handful of non-detects, all quite low.  Other evidence shows the 
non-detects are all from one site....

## Alternate Estimates of Sum of PAHs
We want to focus on analysis of the sum of PAHs, because all PAHs are highly
correlated.  The question is, how do we best handle the non-detects.  Often in
environmental analyses, non-detects are replaced by zero, by the detection
limit, or by half the detection limit, but none of those conventions rests on
strong statistical principles.  We instead implement a method that estimates the
(unobserved) value of non-detects using a conditional mean of censored
observations derived from a maximum likelihood procedure.

The idea is to fit a maximum likelihood model to the data assuming a censored
lognormal distribution.  With a lognormal density in hand, we can estimate a
conditional mean of for "unobserved" observations below a the detection limit by
sampling from the underlying lognormal distribution 1000 times (or more) and
calculating a mean.

We developed functions for implementing this procedure.  Those functions have
been incorporated into a small package, 'LCensMeans' to facilitate use in CBEP
State of the Bay Analyses.

See the help files in the LCensMeans package for more explanation, or read the R
Notebook "Conditional Means of Censored Distributions", where we developed the
basic approach.

The LCenMeans package is in active development in pre-release form, so there is
no guarantee that the user interface will not change.  The following code worked
as of July 2020.

## Applying to the PAHs data
Note the use of "mutate"  after the group_by() so that the data-frame is not 
collapsed to the grouping variables, as it would be by summary().

The calculations involved are random, so if you want to get exactly the same 
results, you need to set the random number seed with set.seed()
```{r treat_censored, fig.height = 8, fig.width = 10}
dat2 <- pah.data.long %>%
  group_by(PAH) %>%
  mutate(LikCensored = sub_cmeans(CONCENTRATION, censored>0)) %>%
  mutate(HalfDL = ifelse(censored>0, CONCENTRATION/2, CONCENTRATION)) %>%
  ungroup()

res2 <- dat2 %>%
  group_by(SAMPLE_ID) %>%
  summarize(LNtotPAH = sum(LikCensored, na.rm=TRUE),  # Not sure why I needed
                                                      # the na.rm=TRUE here...
            halfDLtotPAH = sum(HalfDL),
            totPAH = sum(CONCENTRATION))
```

# Graphic Showing ND Handling
```{r another_graphic} 
ggplot(dat2, aes(CONCENTRATION,LikCensored)) + geom_line() + 
  geom_point(aes(color = censored>0), alpha = 0.5) + 
  geom_abline(intercept = 0, slope= 1, alpha = 0.5, color = 'red') + 
  scale_x_log10(labels = scales::label_number()) +
  scale_y_log10(labels = scales::label_number()) +
  facet_wrap('PAH', scales = 'free') +
  theme_cbep(base_size=6) +
  scale_color_manual(values = cbep_colors2(), name = 'Censored') #+
  #theme(strip.text=element_text(size=5))
```

The first panel shows random variation for a parameter with no detections, but 
variable detection limits. Note that the "corrected" estimates are all small 
and only vary in the fourth decimal place, well below differences that can
possibly matter.

# Total PAHs
## Graphic Comparing ND Methods
```{r total_pah_graphic}
ggplot(res2, aes(x=totPAH))+
  geom_point(aes(y=halfDLtotPAH), color = 'red') +
  geom_point(aes(y=LNtotPAH), fill = 'blue', alpha = 0.5) +
  geom_text(aes(y=LNtotPAH, label = SAMPLE_ID), color = 'orange',
            hjust = 0, nudge_x = 0.1) +

  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_text(aes(x=300000, y=520000, label = '1:1 Line'), angle = 35) +
  geom_hline(yintercept = 4022, color = 'blue', lty=3, size=1) +    #ERL
  geom_text(aes(x=500000, y=4900, label = 'ERL'), color = 'blue') +
  geom_hline(yintercept = 44792, color = 'blue', lty=3, size=1) +     #ERM
  geom_text(aes(x=500000, y=49000, label = 'ERM'), color = 'blue') +
  xlab('Total, Assuming Detection Limit') +
  ylab('Total, Assuming half DL (red) or Max. Lik (orange)') +
  scale_y_log10(label=scales::comma) +
  scale_x_log10(label=scales::comma) +  
  theme_cbep(base_size = 10)
```

So, we see no evidence that which estimate of non-detects we use has any impact 
on our conclusions.  Here, where non-detects are few, maximum likelihood 
estimates are close to the detection limits, which are well below most 
observations.
